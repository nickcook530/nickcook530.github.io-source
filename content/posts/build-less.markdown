Title: Build less, learn more
Date: 2020-05-12
Modified: 2020-05-12
Category: product-management
Tags: product management, software
Slug: build-less-learn-more
Authors: Nick Cook
Summary: *"Don't build it and see if they will come."*
<!-- modified: 2020-05-12 -->

*"Don't build it and see if they will come."*

---
**Lesson Learned:**

"Build it and he will come" a mysterious voice whispers to Kevin Costner in an attempt to get him to build a basesball field in the movie Field of Dreams. I can tell you one thing, that mysterious voice was definitely not coming from a product manager. No PM would suggest spending the resources to build a basesball field without solid evidence that anyone would use it. While it ended up working out for Costner's character we can't rely on mysterious voices in software development and instead have to rely on validating appetite for a feature prior to investing significant resources into building it.

In the [previous post](/minimum-viable-product) we discussed the concept of a minimum viable product (MVP). You might have already started thinking about all the features you could strip out of your next product to build a true MVP but let's get even more "minimum", let's not build anything at all. Remember, the point of an MVP is to collect maximum learning for the least amount of effort. The first thing we probably want to learn is if people are actually interested in our product so let's look at how we can do that with the least amount of effort.

Our goal is to do as little work as possible to get potential users to take an action that indicates interest. These actions come in a variety of forms with different levels of investment required from potential users. The greater the investment, the greater assurance you have that the interest is genuine:

* (Large investment) You can collect payment for your incomplete product. A website like Kickstarter is a great example of this method successfully working. On Kickstarter people actually put their money down to pre-purchase a product that has not yet been made. Getting individuals to provide their credit card information for a yet unmade product is an excellent indicator that your product is filling a need that is not currently being met. 
* (Moderate investment) You collect contact information from potential users, such as email or phone number. While not nearly as exciting as gettin dollar bills people are generally more willing to toss out their email address than their credit card and this lets you build a contact list which you can use to keep people updated and engaged. An example of this would be simply providing the option on your website for prospective users to enter an email address to get updates on a product that has yet to be made and track how many email sign-ups you received.
* (Low investment) You can track clicks on a specific button or area to indicate interest. Maybe you're a mobile game and you introduce a button in the game menu for a new feature that hasn't been developed yet so you can track how many users actually click on that button. It doesn't matter how great your feature was if no one was interested in even clicking to check it out. Compared to taking money or contact information the commitment level from the user is obviously much lower but the advantage is these tests are easier to setup and available in scenarios where you won't actually be charging money for the new product, like a new feature in an existing SaaS product.


**Learning Experience:**

Two similar experiences jumped to my mind when thinking about this blog topic. The first example centers around trying to determine if we should spend development time bringing a feature from our old application interface to our new one. The feature in question was the ability to link two areas of the application together. This linking feature was available all over our application but had varying degrees of usage. Some areas presented easy cuts, others were easy keeps, and some, like the one in this example, didn't have a clear answer based on the qualitative and quantitative data available to us. While our old interface did not support click tracking our new interface did. What we ended up doing was displaying the tab (screen) where the links would be created and seen but put nothing on the tab other than the words "Coming soon". We then tracked how many users were clicking on that tab to see if links were available in this area. Turns out, based on the number of clicks, a lot of users were looking for links in this area. Combined with the information we already had this click data gave us the confidence that this feature was something users wanted in our new interface and we proceeded with development.

The second example is similar to the first but a bit more clever, if I do say so myself. We have a help center that is separate from our application and inside that help center there is a chat option which allows you ask questions and receive live answers to them. The chat functionality was a native feature of the help center so implementing it there was straightforward. While customers who used the help center chat had a positive experience not a lot of customers were using it. We had a hypothesis that customers would use chat a lot more if it was brought inside the application. Unfortunately, moving chat in the application was a significant amount of development work so we started brainstorming how to test our hypothesis with the least amount of work. The solution we came up with was to add a chat button to the application but instead of launching a chat window that button opened an article in our help center that instructed the user how to launch the chat in the help center. We then setup tracking to see how many users clicked on the button and how many additional chats we received. Was this the greatest user experience? Certainly not. But we were able spend a few hours worth of work to validate whether or not we should do days worth of work.